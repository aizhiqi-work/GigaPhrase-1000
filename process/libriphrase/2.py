from datasets import load_from_disk, disable_caching
from collections import Counter
import re
import os
from tqdm import tqdm
from itertools import chain
from datasets import Dataset
import json



# æ–‡æœ¬æ¸…æ´—ï¼šè½¬å°å†™ + å»é™¤éå­—æ¯å­—ç¬¦
def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)  # åªä¿ç•™ a-z å’Œç©ºæ ¼
    return text.strip()

# ç”¨äºæ¸…ç†åŒ…å« â€™ å­—ç¬¦çš„ n-gram
def is_valid_ngram(ngram):
    return not any("â€™" in word or "'" in word for word in ngram)


# å•æ ·æœ¬å¤„ç†å‡½æ•°ï¼šè¿”å›æ¯ä¸ª n-gram çš„åˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²åˆ—è¡¨ï¼‰
def process_example(example):
    transcript = example.get('transcript', example.get('text', ''))
    transcript = clean_text(transcript).split()
    timestamps = json.loads(example.get('words', '[]'))  # é˜²æ­¢ None æˆ–ç©ºå€¼å¯¼è‡´å‡ºé”™

    # æ»‘åŠ¨çª—å£ï¼Œæ£€æŸ¥æ¯ä¸ªç»„åˆçš„æ—¶é—´é•¿åº¦
    valid_windows = []
    window_size = 4  # æœ€å¤šå–4ä¸ªè¯çš„ç»„åˆ

    # éå†æ¯ä¸ªå¯èƒ½çš„çª—å£ç»„åˆ
    for start_idx in range(len(timestamps)):
        for window_len in range(1, window_size + 1):  # çª—å£é•¿åº¦ä¸º1åˆ°4
            end_idx = start_idx + window_len - 1
            if end_idx < len(timestamps):
                # è·å–å½“å‰çª—å£çš„æ—¶é—´
                window_start = timestamps[start_idx]["start"]
                window_end = timestamps[end_idx]["end"]
                duration = window_end - window_start

                # è·å–çª—å£å†…çš„è¯
                window_words = [timestamps[i]["word"] for i in range(start_idx, end_idx + 1)]
                concat_len = len(''.join(window_words))  # æ‹¼æ¥é•¿åº¦

                # æœ€å°é•¿åº¦è¦æ±‚ï¼š1ä¸ªè¯>=2ï¼Œ2ä¸ªè¯>=4ï¼Œ3ä¸ªè¯>=6 ...
                min_len = 2 * window_len

                # æ—¶é—´å’Œé•¿åº¦è¿‡æ»¤
                if 0.5 <= duration <= 2.0 and concat_len >= min_len:
                    valid_windows.append(window_words)

    # åˆ†ç±»ä¿å­˜ n-grams
    ngrams = {'ngram_1': [], 'ngram_2': [], 'ngram_3': [], 'ngram_4': []}
    for window in valid_windows:
        ngram_len = len(window)
        if 1 <= ngram_len <= 4:
            ngram_str = ' '.join(window)
            if is_valid_ngram(window):  # è¿‡æ»¤æ‰åŒ…å« â€™ å­—ç¬¦çš„ n-gram
                ngrams[f'ngram_{ngram_len}'].append(ngram_str)
    
    return ngrams

if __name__ == '__main__':
    print("ğŸ“‚ åŠ è½½æ•°æ®é›†...")
    import pandas as pd

    parquet_file = "/nvme01/openkws/libriphrase/counts/ls-460/list.parquet"
    dataset_df = pd.read_parquet(parquet_file)

    print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…± {len(dataset_df)} ä¸ªæ ·æœ¬")
    
    # å°† pandas DataFrame è½¬æ¢ä¸º Hugging Face Dataset æ ¼å¼
    dataset = Dataset.from_pandas(dataset_df)

    print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…± {len(dataset)} ä¸ªæ ·æœ¬")
    print("ğŸš€ å¼€å§‹é€æ¡å¤„ç†ï¼Œå¤šè¿›ç¨‹æå– n-gram...")

    # ä½¿ç”¨ map å¤šè¿›ç¨‹å¤„ç†ï¼Œé€æ¡ï¼ˆé batchedï¼‰
    result_dataset = dataset.map(
        process_example,
        num_proc=8,                    # å¤šè¿›ç¨‹ï¼Œå¯è°ƒæ•´ä¸ºä½ çš„ CPU æ ¸å¿ƒæ•°
        batched=False,                 # ä¸€æ¡ä¸€æ¡å¤„ç†
        load_from_cache_file=False,    # ç¦ç”¨ç¼“å­˜
        desc="æå– n-gram (é€æ¡)",       # è¿›åº¦æ¡æç¤º
    )

    print("ğŸ“Š æ­£åœ¨ç»Ÿè®¡æ€»é¢‘ç‡...")

    # ä½¿ç”¨ chain.from_iterable é«˜æ•ˆå±•å¹³ list of lists
    final_counters = {
        1: Counter(chain.from_iterable(result_dataset['ngram_1'])),
        2: Counter(chain.from_iterable(result_dataset['ngram_2'])),
        3: Counter(chain.from_iterable(result_dataset['ngram_3'])),
        4: Counter(chain.from_iterable(result_dataset['ngram_4'])),
    }

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs("ngram_results", exist_ok=True)

    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶ï¼Œåªä¿ç•™é¢‘ç‡ >= 2 çš„é¡¹
    print("\nğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœ...")
    for n in range(1, 5):
        filename = f"/nvme01/openkws/libriphrase/counts/ls-460/ngram_{n}.txt"
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        with open(filename, 'w', encoding='utf-8') as f:
            for word, count in final_counters[n].most_common():
                if count >= 10:
                    f.write(f"{word}\t{count}\n")
        print(f"âœ… å·²ä¿å­˜ {n}-gram åˆ° {filename}")
